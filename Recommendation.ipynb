{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token # for creating global objects \n",
    "from spacy.matcher import Matcher # for rule-based matching\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from spacy.language import Language # for building custom pipeline components\n",
    "from spacy.pipeline import EntityRuler \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # initializes the spacy Language model\n",
    "        nlp = spacy.load('en_core_web_lg') # loads our NLP model\n",
    "        nlp.Defaults.stop_words |= {\"hint\", \"Hint\", \"hints\", \"touch\", \"touches\", \"Touch\", \"note\", \"Note\", \"notes\", \"Notes\", \"little\", \"end\", \"thing\", \"palate\", \"Palate\", \"nose\", \"Nose\", \"whisper\", \"whispers\"}\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "        pattern_csk = [{\"LEMMA\": {\"IN\": [\"cask\", \"octave\", \"pipe\", \"puncheon\", \"butt\", \"barrel\", \"hogshead\"]} } ]\n",
    "        patterns = [ {\"label\": \"CSK\", \"pattern\": pattern_csk }]\n",
    "\n",
    "        ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scotch_Recommender:\n",
    "\n",
    "    def __init__(self, corpus = None, dictionary = None, phrase_model = None, full_data = None, lda_mod = None, index_sim = None):\n",
    "\n",
    "        if dictionary == None:\n",
    "            # load pickled dictionary. the default will be the reduced dictionary stored in the dictionary folder.\n",
    "            dictionary_path = \"dictionaries\\\\reduced_gemsimdict_unified.pkl\"\n",
    "            self.dictionary = pickle.load(open(dictionary_path, 'rb'))\n",
    "\n",
    "        if phrase_model == None:\n",
    "            # load phrase model\n",
    "            phraser_path = \"models\\\\phrase_mod.pkl\"\n",
    "            self.phrase_model = pickle.load(open(phraser_path, 'rb'))\n",
    "        \n",
    "        if corpus == None:\n",
    "            #load gensim BoW corpus\n",
    "            corpus_path = \"data\\\\final\\\\descriptor_corpus.pkl\"\n",
    "            self.corpus = pickle.load(open(corpus_path, 'rb'))\n",
    "\n",
    "        if lda_mod == None:\n",
    "            lda_path = \"models\\\\Scotch_LDA.pkl\"\n",
    "            self.scotch_topic_model = pickle.load(open(lda_path, 'rb'))\n",
    " \n",
    "        if index_sim == None:\n",
    "            index_sim_path = \"data\\\\final\\\\index_sim.pkl\"\n",
    "            self.index_sim = pickle.load(open(index_sim_path, 'rb'))\n",
    "\n",
    "        # breakdown of scotch by topic\n",
    "        self.scotch_lda_decomp = self.scotch_topic_model[self.corpus]\n",
    "\n",
    "\n",
    "        if full_data == None:\n",
    "            whisk_token_path = \"data\\\\interim\\\\whisk_unified_tokenized.csv\"\n",
    "            self.full_data = pd.read_csv(whisk_token_path).drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    def get_recommendations(self, name, num_rec = 10):\n",
    "\n",
    "        doc_num = self.full_data[self.full_data['name'] == name].index[0]\n",
    "        sims = self.index_sim[self.scotch_lda_decomp[doc_num]]\n",
    "\n",
    "        sorted_val_df = pd.Series(sims).sort_values(ascending = False).drop(index=doc_num).to_frame(name = \"similarity\")\n",
    "\n",
    "        sorted_val_df['name'] = self.full_data.iloc[sorted_val_df.index].name.values\n",
    "    \n",
    "        # drop whiskies that have the same name and age expression.\n",
    "        sorted_val_df.drop(index = sorted_val_df[sorted_val_df['name'].str.contains(name, flags=re.IGNORECASE, regex=True)].index, inplace = True)\n",
    "\n",
    "\n",
    "        return sorted_val_df.name[0: num_rec]\n",
    "\n",
    "    #------the rest of this class is for taking in a custom whisky desciption and getting recommendations -------------\n",
    "\n",
    "    # now we tokenize the incoming text\n",
    "    def tokenize_text(self, text, *args):\n",
    "\n",
    "        unique_token_set = set(self.dictionary.itervalues())\n",
    "\n",
    "        # construct doc\n",
    "        doc = nlp(text)\n",
    "        # generate token list removing verbs, punctuation, prepositions, stop words, and initial lemmatizing\n",
    "        token_list = [token.lemma_.lower() for token in doc if ( (not token.is_stop) & (not token.is_punct) & (token.pos_ != 'VERB') & (token.dep_ != 'prep') & (token.is_alpha)) ]\n",
    "\n",
    "        # MANUAL LEMMATIZATION\n",
    "        # ----------------------- \n",
    "        doc_str = \" \".join(token_list)\n",
    "\n",
    "        #all of these go to -y endings\n",
    "        regex_search_pattern1 = [r'iness\\b', r'ied\\b', r'iful\\b', r'ifull\\b', r'ifully\\b']\n",
    "        doc_str = re.sub(\"|\".join(regex_search_pattern1) , 'y', doc_str)\n",
    "\n",
    "        # all tokens ending with \"-ness\" or \"-ful\" should just have this ending chopped off.\n",
    "        regex_search_pattern2 = [r'ness\\b', r'ful\\b', r'full\\b']\n",
    "        doc_str = re.sub(\"|\".join(regex_search_pattern2) , '', doc_str)\n",
    "        \n",
    "        #specific word replacement rule\n",
    "        regex_search_pattern3 = r'tannic'\n",
    "        doc_str = re.sub(regex_search_pattern3 , 'tannin', doc_str)\n",
    "\n",
    "        # this is where we deal with -y endings\n",
    "        doc_list = doc_str.split()\n",
    "        regex_search_pattern4 = r\"y\\b\"\n",
    "        regex_search_pattern5 = r\"\\w+[^aeiou][aeiou][^aeiou]\\b\"\n",
    "        \n",
    "        spac_doc_list = []\n",
    "        for token in doc_list:\n",
    "\n",
    "            stemmed_tok = re.sub(regex_search_pattern4, \"\", token)\n",
    "            # check against the gensim dictionary\n",
    "            if stemmed_tok in unique_token_set:\n",
    "                spac_doc_list.append(stemmed_tok)\n",
    "            # if not in dictionary and stem ends with vowel and consonant after stripping y, then add -e to end (smok -> smoke)   \n",
    "            elif (stemmed_tok not in unique_token_set) & (not not re.findall(regex_search_pattern5, stemmed_tok)):\n",
    "                stemmed_tok = stemmed_tok + 'e'\n",
    "                spac_doc_list.append(stemmed_tok)\n",
    "        \n",
    "            else:\n",
    "                spac_doc_list.append(token)\n",
    "        #----FINISH OF MANUAL LEMMATIZATION-----------------------------\n",
    "        # apply trained gensim phrase object on token list\n",
    "        bigram_tokenized = self.phrase_model[spac_doc_list]\n",
    "\n",
    "        return bigram_tokenized\n",
    "\n",
    "    def recommend_from_text(self, text, num_rec = 10):\n",
    "\n",
    "        tokenized = self.tokenize_text(text)\n",
    "        bow_vec = self.dictionary.doc2bow(tokenized)\n",
    "        sims = self.index_sim[self.scotch_topic_model[bow_vec]]\n",
    "\n",
    "        sorted_val_df = pd.Series(sims).sort_values(ascending = False).to_frame(name = \"similarity\")\n",
    "\n",
    "        sorted_val_df['name'] = self.full_data.iloc[sorted_val_df.index].name.values\n",
    "\n",
    "        return sorted_val_df.name[0: num_rec]\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_engine = Scotch_Recommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3682    Jura 26 Year Old 1989 (casks 30739 & 30740) - ...\n",
       "303                                           SPEY Fumare\n",
       "2166    Caol Ila 8 Year Old 2010 (casks 318710 & 31871...\n",
       "3528    Lagavulin 2000 (bottled 2016) Distillers Editi...\n",
       "2921    Lagavulin 2000 (bottled 2016) Pedro Ximénez Ca...\n",
       "1776                           Kilchoman STR Cask Matured\n",
       "7934            Ledaig 21 Year Old Manzanilla Cask Finish\n",
       "6305    Ledaig 22 Year Old 1972 - Lost Bottlings Serie...\n",
       "2219    Caol Ila 12 Year Old 2008 (cask 14251) - Old P...\n",
       "4254    Laphroaig 10 Year Old 2005 (cask 80080) - Dime...\n",
       "6964    Tomintoul 48 Year Old 1967 (casks 150031 & 150...\n",
       "7384          Laphroaig Cairdeas Port Wood Edition (2013)\n",
       "3133                                Ailsa Bay Single Malt\n",
       "6358    Highland Park 21 Year Old 1991 (cask 9200) - D...\n",
       "4709     Bunnahabhain Dràm An Stiùreadair - Feis Ile 2014\n",
       "4049    Springbank 26 Year Old 1990 (cask 096) (Rest &...\n",
       "7131    Caol Ila 7 Year Old 2009 (casks 318823 & 31882...\n",
       "936             Caol Ila 10 Year Old 2009 - Forest Whisky\n",
       "3822    Caol Ila 11 Year Old 2005 (casks 301521, 30152...\n",
       "4749    Kilchoman 5 Year Old 2008 - Single Cask (Maste...\n",
       "353     Annandale Man O’Sword Vintage 2015 - Bourbon C...\n",
       "7171    Ledaig 7 Year Old 2010 (casks 700385 & 700386)...\n",
       "6129    Bunnahabhain 19 Year Old 1990 - Mission (Murra...\n",
       "3963    Ledaig 10 Year Old 2005 (cask 900145) - Cask S...\n",
       "4105    Springbank 21 Year Old 1995 (cask 11366) - Xtr...\n",
       "7320    Springbank 22 Year Old 1993 (cask 227) - The B...\n",
       "1096    Bowmore 14 Year Old 2002 (cask 8) - Benchmark ...\n",
       "5581    Aultmore - Batch 4 (That Boutique-y Whisky Com...\n",
       "4128    Laphroaig 18 Year Old 1997 (cask 3371) - Cask ...\n",
       "1705    Laphroaig 30 Year Old - The Ian Hunter Story B...\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt =\"Behind the obvious peatiness on the nose lurks a beautiful array of scents: smoke, decay, iodine, leather, seawater, charcoal, and wet stone. After opening up, taffy, peanut butter fudge, and sweetly viscous gumminess are present, herbal ocean tones underlying still. On the palate, the first impression is the interplay between sweet and salty. Malted barley and sea salt. Savory, with marked slate and driftwood notes. Match sticks, sulphur, hay, and smoked salt blend together with the ripe sugar elements that define the spirit. It is clear the play between very sweet oak and very smoky peat is what makes this spirit the seminal one that it is today. Sweeter on the palate than expected considering the heft peat brings to whisky, leaves the impression of sea spray and hot breakfast cereal on the finish, fading into just the smoky peat we know it for.\"\n",
    "reco_engine.recommend_from_text(txt, 30)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95508d1f9e276930ee7bf977622471e4c9572bc985693ed5ef89eacdfe75f4af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('sb_cpstone3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
